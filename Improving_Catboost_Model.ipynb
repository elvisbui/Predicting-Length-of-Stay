{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Improving Catboost Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elvisbui/Predicting-Length-of-Stay/blob/master/Improving_Catboost_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2PpOFm3DUtc"
      },
      "source": [
        "#  Improving the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f7oMbZHDUth"
      },
      "source": [
        "### Load Libraries \n",
        "The first step is to load the libraries we will be using. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UeU3g5WDDUtl"
      },
      "source": [
        "import math\n",
        "import pickle\n",
        "import optuna\n",
        "\n",
        "# import data processing and linear algebra libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# import data visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "RANDOM_STATE = 24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox-u71_mDUtw"
      },
      "source": [
        "### Load Data\n",
        "Next, is getting the data and loading it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "YzkBS-QADUty"
      },
      "source": [
        "TRAIN_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data.csv'\n",
        "TEST_DIR = '../input/av-healthcare-analytics-ii/healthcare/test_data.csv'\n",
        "SAMPLE_SUBM = '../input/av-healthcare-analytics-ii/healthcare/sample_sub.csv'\n",
        "TRAIN_DICT_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data_dictionary.csv'\n",
        "\n",
        "def read_csv(*paths: str) -> tuple:\n",
        "    '''\n",
        "    Gets a list of cvs paths and returns all cvs in a tuple\n",
        "\n",
        "            Parameters:\n",
        "                    *paths (tuple of str): A decimal integer\n",
        "\n",
        "            Returns:\n",
        "                    binary_sum (tuple of dataframe): tuple of cvs dataframes\n",
        "    '''\n",
        "    result = []\n",
        "    for dir in paths:\n",
        "        csv = pd.read_csv(dir)\n",
        "        result.append(csv)\n",
        "    return tuple(result)\n",
        "\n",
        "train, test, sample_subm, train_dict = read_csv(TRAIN_DIR, TEST_DIR, SAMPLE_SUBM, TRAIN_DICT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0lVL85jDUuE"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3TucKkoADUuH"
      },
      "source": [
        "stay_encode = {'0-10': 0,\n",
        "               '11-20': 1,\n",
        "               '21-30': 2,\n",
        "               '31-40': 3,\n",
        "               '41-50': 4,\n",
        "               '51-60': 5,\n",
        "               '61-70': 6,\n",
        "               '71-80': 7,\n",
        "               '81-90': 8,\n",
        "               '91-100': 9,\n",
        "               'More than 100 Days': 10}\n",
        "\n",
        "hospital_type_code_encode={'a': 0,\n",
        "                           'b': 1,\n",
        "                           'c': 2,\n",
        "                           'e': 3,\n",
        "                           'd': 4,\n",
        "                           'f': 5,\n",
        "                           'g': 6} \n",
        "\n",
        "hospital_region_encode = {'X': 0, \n",
        "                          'Y': 1, \n",
        "                          'Z': 2}\n",
        "\n",
        "department_encode={'anesthesia': 0,\n",
        "                   'gynecology': 1,\n",
        "                   'radiotherapy': 2,\n",
        "                   'surgery': 3,\n",
        "                   'TB & Chest disease': 4,}\n",
        "\n",
        "word_type_encode = {'P': 0,\n",
        "                    'Q': 1,\n",
        "                    'R': 2,\n",
        "                    'S': 3,\n",
        "                    'T': 4,\n",
        "                    'U': 5}\n",
        "\n",
        "ward_facility_code_encode ={'A': 0, \n",
        "                            'B': 1, \n",
        "                            'C': 2, \n",
        "                            'D': 3, \n",
        "                            'E': 4, \n",
        "                            'F': 5}\n",
        "\n",
        "admission_type_encode = {'Trauma': 0, \n",
        "                         'Emergency': 1, \n",
        "                         'Urgent': 2}\n",
        "\n",
        "illness_encode = {'Minor': 0,\n",
        "                  'Moderate ': 1,\n",
        "                  'Extreme': 2}\n",
        "\n",
        "age_encode = {'0-10': 0,\n",
        "              '11-20': 1,\n",
        "              '21-30': 2,\n",
        "              '31-40': 3,\n",
        "              '41-50': 4,\n",
        "              '51-60': 5,\n",
        "              '61-70': 6,\n",
        "              '71-80': 7,\n",
        "              '81-90': 8,\n",
        "              '91-100': 9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0EPOn7JBDUuR"
      },
      "source": [
        "train['Stay'] = train['Stay'].map(stay_encode)\n",
        "train['Hospital_type_code'] = train['Hospital_type_code'].map(hospital_type_code_encode)\n",
        "train['Hospital_region_code'] = train['Hospital_region_code'].map(hospital_region_encode)\n",
        "train['Department'] = train['Department'].map(department_encode)\n",
        "train['Ward_Type'] = train['Ward_Type'].map(word_type_encode)\n",
        "train['Ward_Facility_Code'] = train['Ward_Facility_Code'].map(ward_facility_code_encode)\n",
        "train['Type of Admission'] = train['Type of Admission'].map(admission_type_encode)\n",
        "train['Severity of Illness'] = train['Severity of Illness'].map(illness_encode)\n",
        "train['Age'] = train['Age'].map(age_encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNKQX9BaDUub"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2p8MM8DDUuc"
      },
      "source": [
        "For featuere engineering, I like to create as much features as I can. This is because poor features might perform while with another poor feature. \n",
        "\n",
        "From exploring the data, we know that there are entries with the same patients. We create features by grouping by patient ID and finding the mean, max, and min of the other features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Lgv15O_KDUuf"
      },
      "source": [
        "def groupby_features(df, col1, col2, fe_stats = ('mean','max','min', 'sum','count', 'count', 'nunique', 'std')):\n",
        "    fe_df = df.copy()\n",
        "    for stat in fe_stats:\n",
        "        fe_df[f'{col1}_{col2}_{stat}'] = df.groupby([col1])[col2].transform(stat)\n",
        "        fe_df[f'{col1}_{col2}_{stat}_diff'] = fe_df[f'{col1}_{col2}_{stat}'] - fe_df[col2]\n",
        "        fe_df[f'{col1}_{col2}_{stat}_div'] = fe_df[f'{col1}_{col2}_{stat}'] / fe_df[col2]\n",
        "    return fe_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hXYIQLhUDUuq"
      },
      "source": [
        "fe_train = train.copy()\n",
        "fe_train = groupby_features(fe_train, 'patientid', 'Admission_Deposit')\n",
        "fe_train = groupby_features(fe_train, 'Severity of Illness', 'Admission_Deposit')\n",
        "fe_train = groupby_features(fe_train, 'Type of Admission', 'Admission_Deposit')\n",
        "fe_train = groupby_features(fe_train, 'Bed Grade', 'Admission_Deposit')\n",
        "fe_train = groupby_features(fe_train, 'Hospital_code', 'Admission_Deposit')\n",
        "fe_train = groupby_features(fe_train, 'Bed Grade', 'Admission_Deposit')\n",
        "fe_train = groupby_features(fe_train, 'patientid', 'Visitors with Patient')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6XMiG7zDUu7"
      },
      "source": [
        "We can do this type of feature engineering with the other types of datas. We first group by a categorical feature, get a numerical feature, and perform some statistics to create a new feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WdVKRYVcDUu9"
      },
      "source": [
        "cat_feats = ['Hospital_code', 'Hospital_type_code', 'City_Code_Hospital',\n",
        "             'Hospital_region_code', 'Department', 'Ward_Type', \n",
        "             'Ward_Facility_Code','patientid', 'City_Code_Patient', \n",
        "             'Type of Admission', 'Severity of Illness']\n",
        "\n",
        "num_feats = ['Available Extra Rooms in Hospital','Bed Grade', \n",
        "             'Visitors with Patient', 'Age', 'Admission_Deposit']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD0vizR5DUvG"
      },
      "source": [
        "Below if the code to create the new features. But since kaggle notebooks have limited ram and space, I will only be using the previous features I created. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "O5Lv399GDUvI"
      },
      "source": [
        "\n",
        "fe_train = train.copy()\n",
        "\n",
        "for cat in cat_feats:\n",
        "    for num in num_feats:\n",
        "        fe_train = groupby_features(fe_train, cat, num)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M096GBxuDUvR"
      },
      "source": [
        "len(fe_train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ScY7ditDUva"
      },
      "source": [
        "# Feature Selection\n",
        "Now that we have lots of features, it is time to select the best ones. There are many methods for feature selection. Here are the steps I will take to select the right features:\n",
        "\n",
        "1. Get feature importances of each feature\n",
        "2. remove feature with lowest importances\n",
        "3. run k-fold cross validation with on the dataset with the removed feature\n",
        "4. if cross validation improves, remove the next least important feature\n",
        "5. if cross validation does not improve, add the feature back in and remove the next least important feature\n",
        "6. run cross validation again and repeat steps 2-5 until we tested all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jjZG9Z91DUvd"
      },
      "source": [
        "cat_features = [0,1,2,3,5,6,7,9]\n",
        "\n",
        "y = fe_train.loc[:,['Stay']]\n",
        "X = fe_train.drop(['Stay','case_id'], axis=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urNVkT14DUvo"
      },
      "source": [
        "### First step is to get the feature importances. I will use catboost to get feature importance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PibwgLJlDUvq"
      },
      "source": [
        "# clf = CatBoostClassifier(task_type=\"GPU\", custom_metric='Accuracy', eval_metric='Accuracy', random_seed=RANDOM_STATE)\n",
        "# history = clf.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_val, y_val), plot=True, early_stopping_rounds=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fuFQlAN-DUvz"
      },
      "source": [
        "# let's save the feature importances for later use\n",
        "feature_importances = \"feature_importances.pkl\"  \n",
        "'''\n",
        "with open(history.feature_importances_, 'wb') as file:  \n",
        "    pickle.dump(fi, file)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3LcEcQ2qDUv-"
      },
      "source": [
        "# Load feature importances\n",
        "with open('../input/features-importance-of-health-care-stay/feature_importances.pkl', 'rb') as file:  \n",
        "    feature_importances = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i2GCjzJBDUwM"
      },
      "source": [
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9KEo99r7DUwT"
      },
      "source": [
        "np.argmin(feature_importances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reLd3dvZDUwe"
      },
      "source": [
        "### K-Fold cross validation \n",
        "\n",
        "For the validation schema, I will use stratified k-hold cross validation with bagging to reduce randomness. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IouOUM9KDUwg"
      },
      "source": [
        "def run_clf(X, y, params={}):\n",
        "    N_FOLDS = 2\n",
        "    N_STARTS = 1\n",
        "\n",
        "    train_score = 0\n",
        "    val_score = 0\n",
        "\n",
        "    for seed in range(N_STARTS):\n",
        "        for n, (train_idx, val_inx) in enumerate(StratifiedKFold(n_splits=N_FOLDS, random_state=RANDOM_STATE, shuffle=True).split(y, y)):\n",
        "            print(f'seed: {seed} ------ fold: {n}')\n",
        "            x_tr, x_val = X.iloc[train_idx], X.iloc[val_inx]\n",
        "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_inx]\n",
        "\n",
        "            model = CatBoostClassifier(**params, task_type=\"GPU\", custom_metric='Accuracy', eval_metric='Accuracy', \n",
        "                                       random_seed=RANDOM_STATE)\n",
        "            history = model.fit(x_tr, y_tr, cat_features=cat_features, eval_set=(x_val, y_val), \n",
        "                                early_stopping_rounds=100, verbose=5)\n",
        "            train_score += model.score(X_train, y_train)/ (N_FOLDS * N_STARTS)\n",
        "            val_score += model.score(x_val, y_val)/(N_FOLDS * N_STARTS)\n",
        "            \n",
        "    return train_score, val_score, history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oUu04buuDUww"
      },
      "source": [
        "train_score, val_score, history = run_clf(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JP2aA4VDUw5"
      },
      "source": [
        "The function below will run catboost on all features, get the feature importances, sort the features by importances, remove the least important feature and runs catboost again on the new dataset. If the cross validation score improves by removing the feature, we will remove the next least important feature and repeat the process again. If the cross validation score decreases, then we know that the feature boosts the score and should not be removed. After testing all the features, the funciton returns the list of all important features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8_v7CHrODUw8"
      },
      "source": [
        "def reduce_features(train, target):\n",
        "    X = train.copy()\n",
        "    y = target.copy()\n",
        "    \n",
        "    prev_ts, prev_cv, prev_hist= run_clf(X, y)\n",
        "    \n",
        "    fi = prev_hist.feature_importances_\n",
        "    fi = [(idx,val) for idx, val in enumerate(fi)]\n",
        "    fi = sorted(fi, key=lambda x: x[1])\n",
        "    \n",
        "    feats = X.columns\n",
        "    \n",
        "    d = t.drop(X.columns[0], axis=1)\n",
        "    \n",
        "    \n",
        "    for (idx, val) in tqdm(fi):\n",
        "        print(f'Checking feature: {feats[idx]}')\n",
        "        X_test = X.drop(feats[idx], axis=1)\n",
        "        curr_ts, curr_cv, curr_hist= run_clf(X_test, y)\n",
        "        \n",
        "        print(f'Previous CV: {prev_cv} ---- Current CV: {curr_cv}')\n",
        "        if curr_cv < prev_cv:\n",
        "            print('Feature Keep')\n",
        "        else:\n",
        "            prev_cv = curr_cv\n",
        "            X = X_test\n",
        "            print('Feature Dropped')\n",
        "            \n",
        "    return X.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uJnoHOfvDUxE"
      },
      "source": [
        "important_features = reduce_features(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYbdOSAQDUxQ"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "To find the right hyperparameters, we can use a search grid but it search grids can take a lot of time to run. We can also use random search grid. It will not find the most optimal hyperparameters, but it will be a lost faster compared to a search grid. We can also use bayesian optimization or a hyperparameter optimization framework. \n",
        "\n",
        "For tuning, I will be using Optuna. \n",
        "\n",
        "Here are the most common hyperpatameters to tune for Catboost. \n",
        "1. Number of trees: iterations\n",
        "    - The more trees we have, we run the risk of overfitting. \n",
        "2. Learning rate: learning_rate\n",
        "    - The faster the learnign rate the faster the model fits our training data.\n",
        "3. Tree depth: depth\n",
        "    - A high number for tree depth can overfit our data. \n",
        "4. Regularizer Coefficient\n",
        "5. Random strength \n",
        "6. Bagging temperature\n",
        "7. Column Subsample\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfDkQEqlDUxR"
      },
      "source": [
        "For hyperparameter tuning, I like manually pick hyperpatameters that would over fit the training data and then use a framework to seach for better hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zWmDqfaLDUxT"
      },
      "source": [
        "\n",
        "def objective(trial): \n",
        "    param = {\n",
        "        \"iterations\": trial.suggest_int(\"iterations\", 300, 1000),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", .01, .3),\n",
        "        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n",
        "        'num_leaves': trial.suggest_int(\"num_leaves\", 25, 100),\n",
        "        'bagging_temperature': trial.suggest_float(\"bagging_temperature\", .1, .5),\n",
        "    }\n",
        "    \n",
        "    train_score, cv_score, history = run_clf(X, y, param)\n",
        "    \n",
        "    return cv_score\n",
        "\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "study.best_params  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yJRlYeLkDUxd"
      },
      "source": [
        "study.best_params  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "u6Ufo-YQDUxo"
      },
      "source": [
        "y = pd.get_dummies(y, columns=['Stay'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM1Prqo8DUxx"
      },
      "source": [
        "# Improved Model\n",
        "Now that we have our features and the right hyperparameters, we can create our improved model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PaJtKSaBDUxz"
      },
      "source": [
        "def run_final_cb(X, y, params={}):\n",
        "    N_FOLDS = 5\n",
        "    N_STARTS = 3\n",
        "\n",
        "    train_score = 0\n",
        "    val_score = 0\n",
        "    \n",
        "    cv_result = pd.get_dummies(y, columns=['Stay'])\n",
        "    cv_result.loc[:, cv_result.columns] = 0\n",
        "\n",
        "    for seed in range(N_STARTS):\n",
        "        for n, (train_idx, val_inx) in enumerate(StratifiedKFold(n_splits=N_FOLDS, random_state=seed, shuffle=True).split(y, y)):\n",
        "            print(f'seed: {seed} ------ fold: {n}')\n",
        "            x_tr, x_val = X.iloc[train_idx], X.iloc[val_inx]\n",
        "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_inx]\n",
        "\n",
        "            model = CatBoostClassifier(learning_rate=0.073, \n",
        "                                       n_estimators=10000,\n",
        "                                       depth=8,\n",
        "                                       bagging_temperature=0.3,\n",
        "                                       task_type=\"GPU\", \n",
        "                                       custom_metric='Accuracy', \n",
        "                                       eval_metric='Accuracy', \n",
        "                                       random_seed=seed)\n",
        "            \n",
        "            history = model.fit(x_tr, y_tr, cat_features=cat_features, eval_set=(x_val, y_val), \n",
        "                                early_stopping_rounds=100, verbose=200)\n",
        "            cv_result.loc[val_inx, cv_result.columns] += model.predict_proba(x_val)\n",
        "            del model, history\n",
        "            \n",
        "    cv_result[:, cv_result.columns] /= (N_FOLDS*N_STARTS)\n",
        "    return cv_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TJFXeLiEDUyG"
      },
      "source": [
        "res = run_final_cb(X, y)\n",
        "log_loss(pd.get_dummies(y, columns=['Stay']),res)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}