{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elvisbui/Predicting-Length-of-Stay/blob/master/Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_yON5GwD6ND"
      },
      "source": [
        "# Ensemble\n",
        "In this notebook, use stacking to get a better result from our neural network and catboost models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qctg9VqWD6Nt"
      },
      "source": [
        "### Load Libraries \n",
        "The first step is to load the libraries we will be using. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "woNCBdjsD6N3"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import gc\n",
        "import math\n",
        "import pickle\n",
        "import optuna\n",
        "\n",
        "from time import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# import data processing and linear algebra libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# import data visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from hyperopt import hp, fmin, atpe, tpe, Trials\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "\n",
        "np.random.seed(24)\n",
        "tf.random.set_seed(24)\n",
        "RANDOM_STATE = 24\n",
        "SEED = 24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPXeS773D6Ok"
      },
      "source": [
        "### Load Data\n",
        "Next, is getting the data and loading it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "3RpQ_K3mD6Om"
      },
      "source": [
        "TRAIN_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data.csv'\n",
        "TEST_DIR = '../input/av-healthcare-analytics-ii/healthcare/test_data.csv'\n",
        "SAMPLE_SUBM = '../input/av-healthcare-analytics-ii/healthcare/sample_sub.csv'\n",
        "TRAIN_DICT_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data_dictionary.csv'\n",
        "\n",
        "def read_csv(*paths: str) -> tuple:\n",
        "    '''\n",
        "    Gets a list of cvs paths and returns all cvs in a tuple\n",
        "\n",
        "            Parameters:\n",
        "                    *paths (tuple of str): A decimal integer\n",
        "\n",
        "            Returns:\n",
        "                    binary_sum (tuple of dataframe): tuple of cvs dataframes\n",
        "    '''\n",
        "    result = []\n",
        "    for dir in paths:\n",
        "        csv = pd.read_csv(dir)\n",
        "        result.append(csv)\n",
        "    return tuple(result)\n",
        "\n",
        "train, test, sample_subm, train_dict = read_csv(TRAIN_DIR, TEST_DIR, SAMPLE_SUBM, TRAIN_DICT_DIR)\n",
        "\n",
        "# The training set for catboost\n",
        "train_cb = train.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xkWHokDAD6Oy"
      },
      "source": [
        "target = train.loc[:,['Stay']]\n",
        "train = train.drop(['Stay'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qwYAQqLCD6O9"
      },
      "source": [
        "cat_columns = ['Hospital_code', 'Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code',\n",
        "               'Department', 'Ward_Type', 'Ward_Facility_Code', 'City_Code_Patient']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fj2O3__YD6PI"
      },
      "source": [
        "stay_encode = {'0-10': 0,\n",
        "               '11-20': 1,\n",
        "               '21-30': 2,\n",
        "               '31-40': 3,\n",
        "               '41-50': 4,\n",
        "               '51-60': 5,\n",
        "               '61-70': 6,\n",
        "               '71-80': 7,\n",
        "               '81-90': 8,\n",
        "               '91-100': 9,\n",
        "               'More than 100 Days': 10}\n",
        "\n",
        "admission_type_encode = {'Trauma': 0, \n",
        "                         'Emergency': 1, \n",
        "                         'Urgent': 2}\n",
        "\n",
        "illness_encode = {'Minor': 0,\n",
        "                  'Moderate': 1,\n",
        "                  'Extreme': 2}\n",
        "\n",
        "age_encode = {'0-10': 0,\n",
        "              '11-20': 1,\n",
        "              '21-30': 2,\n",
        "              '31-40': 3,\n",
        "              '41-50': 4,\n",
        "              '51-60': 5,\n",
        "              '61-70': 6,\n",
        "              '71-80': 7,\n",
        "              '81-90': 8,\n",
        "              '91-100': 9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "78ZhvIPCD6PT"
      },
      "source": [
        "hospital_type_code_encode={'a': 0,\n",
        "                           'b': 1,\n",
        "                           'c': 2,\n",
        "                           'e': 3,\n",
        "                           'd': 4,\n",
        "                           'f': 5,\n",
        "                           'g': 6} \n",
        "\n",
        "hospital_region_encode = {'X': 0, \n",
        "                          'Y': 1, \n",
        "                          'Z': 2}\n",
        "\n",
        "department_encode={'anesthesia': 0,\n",
        "                   'gynecology': 1,\n",
        "                   'radiotherapy': 2,\n",
        "                   'surgery': 3,\n",
        "                   'TB & Chest disease': 4,}\n",
        "\n",
        "word_type_encode = {'P': 0,\n",
        "                    'Q': 1,\n",
        "                    'R': 2,\n",
        "                    'S': 3,\n",
        "                    'T': 4,\n",
        "                    'U': 5}\n",
        "\n",
        "ward_facility_code_encode ={'A': 0, \n",
        "                            'B': 1, \n",
        "                            'C': 2, \n",
        "                            'D': 3, \n",
        "                            'E': 4, \n",
        "                            'F': 5}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kAMWVFVcD6Pf"
      },
      "source": [
        "# preprocess for nueral networks\n",
        "def preprocess_nn(df):\n",
        "    df['Bed Grade'].fillna(train['Bed Grade'].median(), inplace=True)\n",
        "    df['City_Code_Patient'] = df.groupby(['City_Code_Hospital', 'Hospital_type_code','Department'], sort=False)['City_Code_Patient'].apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
        "    df = df.astype({'Hospital_code':'category', 'Hospital_type_code':'category', \n",
        "                     'City_Code_Hospital':'category', \n",
        "                     'Hospital_region_code':'category','Department':'category', \n",
        "                     'Ward_Type':'category', 'Ward_Facility_Code':'category', \n",
        "                     'City_Code_Patient':'category'})\n",
        "    df_one_hot = pd.get_dummies(df, columns = cat_columns)\n",
        "    \n",
        "    df_one_hot['Type of Admission'] = df_one_hot['Type of Admission'].map(admission_type_encode)\n",
        "    df_one_hot['Severity of Illness'] = df_one_hot['Severity of Illness'].map(illness_encode)\n",
        "    df_one_hot['Age'] = df_one_hot['Age'].map(age_encode)\n",
        "    df_one_hot['patient_deposit_mean'] = df.groupby(['patientid'])['Admission_Deposit'].transform('count')\n",
        "    \n",
        "    return df_one_hot.drop(['case_id', 'patientid'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3u3u9kbUD6Pq"
      },
      "source": [
        "def groupby_features(df, col1, col2, fe_stats = ('mean','max','min', 'sum','count', 'count', 'nunique', 'std')):\n",
        "    fe_df = df.copy()\n",
        "    for stat in fe_stats:\n",
        "        fe_df[f'{col1}_{col2}_{stat}'] = df.groupby([col1])[col2].transform(stat)\n",
        "        fe_df[f'{col1}_{col2}_{stat}_diff'] = fe_df[f'{col1}_{col2}_{stat}'] - fe_df[col2]\n",
        "        fe_df[f'{col1}_{col2}_{stat}_div'] = fe_df[f'{col1}_{col2}_{stat}'] / fe_df[col2]\n",
        "    return fe_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YdmApaL7D6P1"
      },
      "source": [
        "def preprocess_cb(df):\n",
        "    # preprocess for catboost\n",
        "    df['Hospital_type_code'] = df['Hospital_type_code'].map(hospital_type_code_encode)\n",
        "    df['Hospital_region_code'] = df['Hospital_region_code'].map(hospital_region_encode)\n",
        "    df['Department'] = df['Department'].map(department_encode)\n",
        "    df['Ward_Type'] = df['Ward_Type'].map(word_type_encode)\n",
        "    df['Ward_Facility_Code'] = df['Ward_Facility_Code'].map(ward_facility_code_encode)\n",
        "    df['Type of Admission'] = df['Type of Admission'].map(admission_type_encode)\n",
        "    df['Severity of Illness'] = df['Severity of Illness'].map(illness_encode)\n",
        "    df['Age'] = df['Age'].map(age_encode)\n",
        "    \n",
        "    fe_train = df.copy()\n",
        "    fe_train = groupby_features(fe_train, 'patientid', 'Admission_Deposit')\n",
        "    fe_train = groupby_features(fe_train, 'Severity of Illness', 'Admission_Deposit')\n",
        "    fe_train = groupby_features(fe_train, 'Type of Admission', 'Admission_Deposit')\n",
        "    fe_train = groupby_features(fe_train, 'Bed Grade', 'Admission_Deposit')\n",
        "    fe_train = groupby_features(fe_train, 'Hospital_code', 'Admission_Deposit')\n",
        "    fe_train = groupby_features(fe_train, 'Bed Grade', 'Admission_Deposit')\n",
        "    fe_train = groupby_features(fe_train, 'patientid', 'Visitors with Patient')\n",
        "    \n",
        "    return fe_train.drop(['case_id'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1ciHIH5kD6P-"
      },
      "source": [
        "X_nn = preprocess_nn(train.copy())\n",
        "test_nn = preprocess_nn(test.copy())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_nn = scaler.fit_transform(X_nn)\n",
        "X_nn = pd.DataFrame(X_nn)\n",
        "\n",
        "test_nn = scaler.transform(test_nn)\n",
        "test_nn = pd.DataFrame(test_nn)\n",
        "\n",
        "y = target.copy()\n",
        "y['Stay'] = y['Stay'].map(stay_encode)\n",
        "# used for splitting using StratifiedKFold\n",
        "# StratifiedKFold cannot split with one-hot encoded labels\n",
        "skf_y = y['Stay']\n",
        "y = pd.get_dummies(target, columns=['Stay'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "loOKdGyPD6QJ"
      },
      "source": [
        "def improved_nn(num_cols):\n",
        "    model = tf.keras.Sequential([      \n",
        "    tf.keras.layers.Input(num_cols),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(256, activation='relu')),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(128, activation='relu')),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(11, activation='softmax'))\n",
        "    ])\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam())\n",
        "                  , metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cZCOCoirD6QW"
      },
      "source": [
        "def run_final_nn(X, y, test):\n",
        "\n",
        "    N_SPLITS=5\n",
        "    N_STARTS=3 \n",
        "    \n",
        "    cv_result = y.copy()\n",
        "    cv_result.loc[:, y.columns] = 0\n",
        "    \n",
        "    test_result = np.zeros((len(test), 11))\n",
        "    \n",
        "    historys = {}\n",
        "\n",
        "    skf = StratifiedKFold(n_splits = N_SPLITS, random_state = RANDOM_STATE, shuffle = True)\n",
        "    \n",
        "    for seed in range(N_STARTS):\n",
        "        for n, (train_ind, val_ind) in enumerate(skf.split(skf_y, skf_y)):\n",
        "\n",
        "            print(f'Seed: {seed} ------------- Fold:{n}')\n",
        "\n",
        "            x_tr, x_val = X.values[train_ind], X.values[val_ind]\n",
        "            y_tr, y_val = y.values[train_ind], y.values[val_ind]\n",
        "\n",
        "            model = improved_nn(X.shape[1])\n",
        "\n",
        "            checkpoint_path = f'Seed:{seed}-Fold:{n}.hdf5'\n",
        "\n",
        "            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-5, \n",
        "                                               patience=3, verbose=0, mode='min')\n",
        "\n",
        "            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
        "                                         verbose = 0, save_best_only = True, \n",
        "                                         save_weights_only = True, mode = 'min')\n",
        "\n",
        "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
        "                                  restore_best_weights=True, \n",
        "                                  patience= 5, verbose = 0)\n",
        "\n",
        "            history = model.fit(x_tr, y_tr, \n",
        "                                validation_data=(x_val, y_val), \n",
        "                                epochs=50, \n",
        "                                batch_size=128,\n",
        "                                callbacks=[reduce_lr_loss, cb_checkpt, early],\n",
        "                                verbose=2)\n",
        "\n",
        "            hist = pd.DataFrame(history.history)\n",
        "\n",
        "            model.load_weights(checkpoint_path)\n",
        "\n",
        "            cv_result.loc[val_ind, y.columns] += model.predict(x_val)\n",
        "            test_result += model.predict(test)\n",
        "\n",
        "            K.clear_session()\n",
        "            del model, history, hist\n",
        "            gc.collect()\n",
        "    \n",
        "    cv_result.loc[:, y.columns] /= (N_STARTS)\n",
        "    test_result /= (N_STARTS*N_SPLITS)\n",
        "    return cv_result, test_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c70IVBMpD6Qg"
      },
      "source": [
        "nn_cv_result, nn_test_result = run_final_nn(X_nn, y, test_nn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0PlNsuw7D6Q1"
      },
      "source": [
        "y_cb = target.copy()\n",
        "y_cb['Stay'] = y_cb['Stay'].map(stay_encode)\n",
        "X_cb = preprocess_cb(train.copy())\n",
        "test_cb = preprocess_cb(test.copy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "V9Gt2J7FD6Q_"
      },
      "source": [
        "cat_features = [0,1,2,3,5,6,7,9]\n",
        "\n",
        "def run_final_cb(X, y, test):\n",
        "    N_FOLDS = 5\n",
        "    N_STARTS = 3\n",
        "\n",
        "    train_score = 0\n",
        "    val_score = 0\n",
        "    \n",
        "    cv_result = pd.get_dummies(y, columns=['Stay'])\n",
        "    cv_result.loc[:, cv_result.columns] = 0\n",
        "    \n",
        "    test_result = np.zeros((len(test), 11))\n",
        "\n",
        "    for seed in range(N_STARTS):\n",
        "        for n, (train_idx, val_inx) in enumerate(StratifiedKFold(n_splits=N_FOLDS, random_state=seed, shuffle=True).split(y, y)):\n",
        "            print(f'seed: {seed} ------ fold: {n}')\n",
        "            x_tr, x_val = X.iloc[train_idx], X.iloc[val_inx]\n",
        "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_inx]\n",
        "\n",
        "            model = CatBoostClassifier(learning_rate=0.073, \n",
        "                                       n_estimators=10000,\n",
        "                                       depth=8,\n",
        "                                       bagging_temperature=0.3,\n",
        "                                       task_type=\"GPU\", \n",
        "                                       custom_metric='Accuracy', \n",
        "                                       eval_metric='Accuracy', \n",
        "                                       random_seed=seed)\n",
        "            \n",
        "            history = model.fit(x_tr, y_tr, cat_features=cat_features, eval_set=(x_val, y_val), \n",
        "                                early_stopping_rounds=100, verbose=200)\n",
        "            cv_result.loc[val_inx, cv_result.columns] += model.predict_proba(x_val)\n",
        "            test_result += model.predict_proba(test)\n",
        "            del model, history\n",
        "            \n",
        "    cv_result.loc[:, cv_result.columns] /= (N_STARTS)\n",
        "    test_result /= (N_STARTS*N_SPLITS)\n",
        "    return cv_result, test_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qou5kpasD6RM"
      },
      "source": [
        "catboost_cv_result, catboost_test_result = run_final_cb(X_cb, y_cb, test_cb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y26DA0FD6RX"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hQWKNx_tD6RY"
      },
      "source": [
        "def concat_results(nn_result, cb_result):\n",
        "    return pd.concat([pd.DataFrame(nn_result,columns=[col for col in range(0,11)]),\n",
        "                           pd.DataFrame(cb_result,columns=[col for col in range(11,22)])],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LHxN8j_5D6Rq"
      },
      "source": [
        "cv_concat = concat_results(nn_cv_result.to_numpy(), catboost_cv_result.to_numpy())\n",
        "test_results_concat = concat_results(nn_test_result, catboost_test_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g_vYCn7rD6SI"
      },
      "source": [
        "def stack_nn(num_cols):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(num_cols),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(11, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YI1ZVRhwD6SS"
      },
      "source": [
        "def run_stacking(X, y, test):\n",
        "    N_SEEDS = 3\n",
        "    N_FOLDS = 5\n",
        "    \n",
        "    cv_result = y.copy()\n",
        "    cv_result.loc[:, y.columns] = 0\n",
        "    \n",
        "    test_result = np.zeros((len(test), 11))\n",
        "    \n",
        "    for seed in range(N_STARTS):\n",
        "        for n, (train_ind, val_ind) in enumerate(skf.split(skf_y, skf_y)):\n",
        "\n",
        "            print(f'Seed: {seed} ------------- Fold:{n}')\n",
        "\n",
        "            x_tr, x_val = X.values[train_ind], X.values[val_ind]\n",
        "            y_tr, y_val = y.values[train_ind], y.values[val_ind]\n",
        "\n",
        "            model = stack_nn(X.shape[1])\n",
        "\n",
        "            checkpoint_path = f'Stacking----Seed:{seed}-Fold:{n}.hdf5'\n",
        "\n",
        "            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-5, \n",
        "                                               patience=3, verbose=0, mode='min')\n",
        "\n",
        "            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
        "                                         verbose = 0, save_best_only = True, \n",
        "                                         save_weights_only = True, mode = 'min')\n",
        "\n",
        "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
        "                                  restore_best_weights=True, \n",
        "                                  patience= 5, verbose = 0)\n",
        "\n",
        "            history = model.fit(x_tr, y_tr, \n",
        "                                validation_data=(x_val, y_val), \n",
        "                                epochs=50, \n",
        "                                batch_size=128,\n",
        "                                callbacks=[reduce_lr_loss, cb_checkpt, early],\n",
        "                                verbose=2)\n",
        "\n",
        "            hist = pd.DataFrame(history.history)\n",
        "\n",
        "            model.load_weights(checkpoint_path)\n",
        "\n",
        "            cv_result.loc[val_ind, y.columns] += model.predict(x_val)\n",
        "            test_result += model.predict(test)\n",
        "\n",
        "            K.clear_session()\n",
        "            del model, history, hist\n",
        "            gc.collect()\n",
        "    \n",
        "    cv_result.loc[:, y.columns] /= (N_STARTS)\n",
        "    test_result /= (N_STARTS*N_SPLITS)\n",
        "    return cv_result, test_result\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v1nKsRI2D6So"
      },
      "source": [
        "cv_result, test_result = run_stacking(cv_concat, y, test_results_concat)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}