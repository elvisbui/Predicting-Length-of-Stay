{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "EDA and Baseline Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elvisbui/Predicting-Length-of-Stay/blob/master/EDA_and_Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPBDvZMvC31G"
      },
      "source": [
        "#  Exploratory Data Analysis\n",
        "\n",
        "In this notebook, I will be doing a basic analysis of healthcare data and creating a baseline model from this dataset: https://www.kaggle.com/nehaprabhavalkar/av-healthcare-analytics-ii\n",
        "\n",
        "I will start with a simple pipeline, check that it runs properly, and slowly add more complexity to it. This way of program makes it easy to debug and fix errors. The baseline model will give me insight on how other models compare and how to improve my models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMv68ZuNC31P"
      },
      "source": [
        "### Load Libraries \n",
        "The first step is to load the libraries we will be using. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zcAngAq1C31S"
      },
      "source": [
        "import math\n",
        "\n",
        "# import data processing and linear algebra libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# import data visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Setting seeds to remove randomness \n",
        "RANDOM_STATE = 24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "747lOp4pC31d"
      },
      "source": [
        "### Load Data\n",
        "Next is getting the data and loading it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "j6NADwGjC31h"
      },
      "source": [
        "TRAIN_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data.csv'\n",
        "TEST_DIR = '../input/av-healthcare-analytics-ii/healthcare/test_data.csv'\n",
        "SAMPLE_SUBM = '../input/av-healthcare-analytics-ii/healthcare/sample_sub.csv'\n",
        "TRAIN_DICT_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data_dictionary.csv'\n",
        "\n",
        "def read_csv(*paths: str) -> tuple:\n",
        "    '''\n",
        "    Gets a list of cvs paths and returns all cvs in a tuple\n",
        "\n",
        "            Parameters:\n",
        "                    *paths (tuple of str): A decimal integer\n",
        "\n",
        "            Returns:\n",
        "                    binary_sum (tuple of dataframe): tuple of cvs dataframes\n",
        "    '''\n",
        "    result = []\n",
        "    for dir in paths:\n",
        "        csv = pd.read_csv(dir)\n",
        "        result.append(csv)\n",
        "    return tuple(result)\n",
        "\n",
        "train, test, sample_subm, train_dict = read_csv(TRAIN_DIR, TEST_DIR, SAMPLE_SUBM, TRAIN_DICT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29L6j3x0C31r"
      },
      "source": [
        "# Looking at the data\n",
        "Let's not take a peek at the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "30ZWk1NCC31s"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7kBPlF0gC312"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0KnHezzxC32A"
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EXs5_OgC32K"
      },
      "source": [
        "By looking at the data types of each column, I see that we are dealing with a lot of categorical data.\n",
        "This leads me to think that CatBoost might be well suited for this data. Let's see how many unique values each column has. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8lggqbr1C32M"
      },
      "source": [
        "train.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhevzTBlC32W"
      },
      "source": [
        "Let's now try to understand which categorical data is nominal, and which is ordinal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1PjhcMBvC32Y"
      },
      "source": [
        "train_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ba8ez2r8C32n"
      },
      "source": [
        "train.Hospital_code.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu-qphNGC32v"
      },
      "source": [
        "While the hospital code are numerical, I do not believe there is a ranking to them. A hospital with a top 1 does not mean it is better than a hospital with a code 20, and vice versa. \n",
        "\n",
        "Let's keep checking the other columns. \n",
        "Hospital_type_code, City_Code_Hospital, Hospital_region_code, Department, Ward_Facility_Code, and Ward_Type are all nominal data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mX7nalpwC32x"
      },
      "source": [
        "train['Type of Admission'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WPRvpP7oC325"
      },
      "source": [
        "train['Severity of Illness'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2kNeOipBC33C"
      },
      "source": [
        "train['Bed Grade'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiAJ-BGyC33K"
      },
      "source": [
        "Type of Admission, Severity of Illness, and Bed Grade are all ordinal data. This will help us decide on which encoding to use for which feature. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O3y-DtCC33L"
      },
      "source": [
        "# Checking for missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "X3hYGGPwC33N"
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8cs4EIvC33W"
      },
      "source": [
        "It looks like only bed grade and city code patient have missing values. We could fill in the missing bed grade values with the median of all bed grades. Since there are return patients, we can try to get the missing city code patient values from other rows with the same patient id. And if there are no other rows with the same patient id, we can try to find rows with similiar data and use their city patient code to replace the missing values. \n",
        "\n",
        "We can also use a boost tree since they deal well with missing values. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UusDiNeyC33X"
      },
      "source": [
        "# Check for outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QsSetbmPC33Z"
      },
      "source": [
        "train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEvubQfGC33h"
      },
      "source": [
        "Extra rooms, visitors, and admission deposit seems like they might have outliers. The problem with determine outliers is that we need to determine if they are accutally outliers or just extreme entries that need to be taken into consideration for our models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TmSZ_rHbC33j"
      },
      "source": [
        "train.boxplot('Available Extra Rooms in Hospital')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "swU0FBaMC33r"
      },
      "source": [
        "train.boxplot('Visitors with Patient')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8aP86hXdC33z"
      },
      "source": [
        "train.boxplot('Admission_Deposit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2X7XUrJC339"
      },
      "source": [
        "Dealing with outliers is tricky because outliers might help improve the performance of our models. Looking at the boxplot, it seems like 'Available Extra Rooms in Hospital' have few outliers, but we should not remove them from the data because available extra rooms change a lot in the real world. The same can be said for visitors and admsision deposit. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbnNde0PC33_"
      },
      "source": [
        "Let's check out some correlations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Gy1xwMRrC34B"
      },
      "source": [
        "def graph_groupby_values(df, column1, column2):\n",
        "    col1_names = df[column1].unique().tolist()\n",
        "    col1_names.sort()\n",
        "    fig, axes = plt.subplots(nrows=math.ceil(len(col1_names)/2), ncols=2)\n",
        "    for idx, name in enumerate(col1_names):\n",
        "        gb = train.loc[train[column1] == name].groupby(column2)\n",
        "        gb.size().plot(kind='bar', ax=axes[idx//2, idx%2], figsize=(14, 25), title=column1+': '+ name)\n",
        "    fig.tight_layout(pad=3.0)\n",
        "        \n",
        "graph_groupby_values(train, 'Age', 'Stay')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X98hW3bKC34J"
      },
      "source": [
        "# Preprocessing\n",
        "Let's label encode some of the features for Catboost. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yyI_PPWcC34K"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VF9CXZnC34T"
      },
      "source": [
        "We can use Pandas's factorize and scikit-learn's LabelEncoder functions for label encoding, but for ordinal features like 'Stay' and 'Type of Admission', we need more control over the mapping. And since we are using a tree boost, we do not need to scale the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qHepC-puC34V"
      },
      "source": [
        "stay_encode = {'0-10': 0,\n",
        "               '11-20': 1,\n",
        "               '21-30': 2,\n",
        "               '31-40': 3,\n",
        "               '41-50': 4,\n",
        "               '51-60': 5,\n",
        "               '61-70': 6,\n",
        "               '71-80': 7,\n",
        "               '81-90': 8,\n",
        "               '91-100': 9,\n",
        "               'More than 100 Days': 10}\n",
        "\n",
        "hospital_type_code_encode={'a': 0,\n",
        "                           'b': 1,\n",
        "                           'c': 2,\n",
        "                           'e': 3,\n",
        "                           'd': 4,\n",
        "                           'f': 5,\n",
        "                           'g': 6} \n",
        "\n",
        "hospital_region_encode = {'X': 0, \n",
        "                          'Y': 1, \n",
        "                          'Z': 2}\n",
        "\n",
        "department_encode={'anesthesia': 0,\n",
        "                   'gynecology': 1,\n",
        "                   'radiotherapy': 2,\n",
        "                   'surgery': 3,\n",
        "                   'TB & Chest disease': 4,}\n",
        "\n",
        "word_type_encode = {'P': 0,\n",
        "                    'Q': 1,\n",
        "                    'R': 2,\n",
        "                    'S': 3,\n",
        "                    'T': 4,\n",
        "                    'U': 5}\n",
        "\n",
        "ward_facility_code_encode ={'A': 0, \n",
        "                            'B': 1, \n",
        "                            'C': 2, \n",
        "                            'D': 3, \n",
        "                            'E': 4, \n",
        "                            'F': 5}\n",
        "\n",
        "admission_type_encode = {'Trauma': 0, \n",
        "                         'Emergency': 1, \n",
        "                         'Urgent': 2}\n",
        "\n",
        "illness_encode = {'Minor': 0,\n",
        "                  'Moderate ': 1,\n",
        "                  'Extreme': 2}\n",
        "\n",
        "age_encode = {'0-10': 0,\n",
        "              '11-20': 1,\n",
        "              '21-30': 2,\n",
        "              '31-40': 3,\n",
        "              '41-50': 4,\n",
        "              '51-60': 5,\n",
        "              '61-70': 6,\n",
        "              '71-80': 7,\n",
        "              '81-90': 8,\n",
        "              '91-100': 9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b7LUZ006C34h"
      },
      "source": [
        "train['Stay'] = train['Stay'].map(stay_encode)\n",
        "train['Hospital_type_code'] = train['Hospital_type_code'].map(hospital_type_code_encode)\n",
        "train['Hospital_region_code'] = train['Hospital_region_code'].map(hospital_region_encode)\n",
        "train['Department'] = train['Department'].map(department_encode)\n",
        "train['Ward_Type'] = train['Ward_Type'].map(word_type_encode)\n",
        "train['Ward_Facility_Code'] = train['Ward_Facility_Code'].map(ward_facility_code_encode)\n",
        "train['Type of Admission'] = train['Type of Admission'].map(admission_type_encode)\n",
        "train['Severity of Illness'] = train['Severity of Illness'].map(illness_encode)\n",
        "train['Age'] = train['Age'].map(age_encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IgGrGo4WC34r"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whetBf6RC342"
      },
      "source": [
        "Now that we have our data encoded, let's check the correlations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FdHWNMr6C345"
      },
      "source": [
        "import seaborn as sns\n",
        "corr = train.corr()\n",
        "sns.heatmap(corr, \n",
        "            xticklabels=train.columns.values,\n",
        "            yticklabels=train.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PCRgGWQC35F"
      },
      "source": [
        "From the heatmap, I can see that amount of visitors, age, and severity of illness has a relatively high correlation with stay length. We will use this insight when we create new features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU_9Z079C35K"
      },
      "source": [
        "# Let's now create a baseline model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVHl5CigC35L"
      },
      "source": [
        "We need to seperate the target column from the training data. And then we need to split the data for training and for testing. We are not using k fold cross validation in this basemodel because we just want to get the baseline model to run and get insight from the results. \n",
        "\n",
        "For catboost, we need the indices of categorical features. Patient ID might seem numerical at first, but since there is no ranking between the the different ID numbers, the feature is nominal and so it is categorical. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-h_kVwm7C35N"
      },
      "source": [
        "cat_features = [0,1,2,3,5,6,7,9]\n",
        "\n",
        "y = train.loc[:,['Stay']]\n",
        "X = train.drop(['Stay','case_id'], axis=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FiAwzeOVC35b"
      },
      "source": [
        "X.iloc[:,cat_features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "10NuLP7-C35j"
      },
      "source": [
        "clf = CatBoostClassifier(task_type=\"GPU\", custom_metric='Accuracy', eval_metric='Accuracy', random_seed=RANDOM_STATE)\n",
        "history = clf.fit(X_train, y_train, cat_features=cat_features, eval_set=(X_val, y_val), plot=True, early_stopping_rounds=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAZ7DBJOC35q"
      },
      "source": [
        "# Review Model Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q_2tWnPjC35s"
      },
      "source": [
        "train_score = clf.score(X_train, y_train) # train (learn) score\n",
        "val_score = clf.score(X_val, y_val) # val (test) score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9c_RBrTeC35y"
      },
      "source": [
        "print('Train Score:', train_score)\n",
        "print('Validation Score:', val_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5fkewjtC359"
      },
      "source": [
        "We can see that our training score is a little higher than our validation score. This could mean that the model is overfitting the training data a little. Let's take a closer look at the results of our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M6my4XldC35_"
      },
      "source": [
        "def report_model_score(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    corr = confusion_matrix(y, y_pred)\n",
        "    print('Confusion Matrix: \\n', corr)\n",
        "    print('\\nAccuracy Score: \\n', accuracy_score(y, y_pred))\n",
        "    # print('\\nROC AUC: \\n', roc_auc_score(y, y_pred))\n",
        "    print('\\nReport: \\n', classification_report(y,y_pred))\n",
        "    y_labels = range(0,11)\n",
        "    sns.heatmap(corr, xticklabels=y_labels, yticklabels=y_labels)\n",
        "    # plot_confusion_matrix(model, X, y)\n",
        "    # plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Wpj_PxrzC36J"
      },
      "source": [
        "report_model_score(clf, X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McuU7eZ_C36Q"
      },
      "source": [
        "# Error Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "_iMBe15XC36R"
      },
      "source": [
        "We see that our model can predict the length of stay at 1, 2, 3, and 5 well. What is strange is that the length of stay at 4 is getting predict to be 2. Stay in group 4 does poorly overall. "
      ]
    }
  ]
}