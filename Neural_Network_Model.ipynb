{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Neural-Network-Model.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elvisbui/Predicting-Length-of-Stay/blob/master/Neural_Network_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvwmCA84DohX"
      },
      "source": [
        "#  Neural Networks\n",
        "In this notebook, I will be creating a deep neural network to predict the amount a patient will stay at a hospital. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIKCYkWQDohb"
      },
      "source": [
        "### Load Libraries \n",
        "The first step is to load the libraries we will be using. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_eMeN0pIDohg"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import gc\n",
        "import math\n",
        "import pickle\n",
        "import optuna\n",
        "\n",
        "from time import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "# import data processing and linear algebra libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# import data visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from hyperopt import hp, fmin, atpe, tpe, Trials\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "\n",
        "np.random.seed(24)\n",
        "tf.random.set_seed(24)\n",
        "RANDOM_STATE = 24\n",
        "SEED = 24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzjgCF0HDohz"
      },
      "source": [
        "### Load Data\n",
        "Next, is getting the data and loading it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "_xCC2a5sDoh2"
      },
      "source": [
        "TRAIN_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data.csv'\n",
        "TEST_DIR = '../input/av-healthcare-analytics-ii/healthcare/test_data.csv'\n",
        "SAMPLE_SUBM = '../input/av-healthcare-analytics-ii/healthcare/sample_sub.csv'\n",
        "TRAIN_DICT_DIR = '../input/av-healthcare-analytics-ii/healthcare/train_data_dictionary.csv'\n",
        "\n",
        "def read_csv(*paths: str) -> tuple:\n",
        "    '''\n",
        "    Gets a list of cvs paths and returns all cvs in a tuple\n",
        "\n",
        "            Parameters:\n",
        "                    *paths (tuple of str): A decimal integer\n",
        "\n",
        "            Returns:\n",
        "                    binary_sum (tuple of dataframe): tuple of cvs dataframes\n",
        "    '''\n",
        "    result = []\n",
        "    for dir in paths:\n",
        "        csv = pd.read_csv(dir)\n",
        "        result.append(csv)\n",
        "    return tuple(result)\n",
        "\n",
        "train, test, sample_subm, train_dict = read_csv(TRAIN_DIR, TEST_DIR, SAMPLE_SUBM, TRAIN_DICT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMsVzFt5Doiw"
      },
      "source": [
        "# Preprocessing\n",
        "Preprocessing data for neural networks is different compared to preprocessing data for a gradient boosting tree.\n",
        "Let's first deal with missing data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ya_q3i02DojD"
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz1nrHX7Dojm"
      },
      "source": [
        "We see that 'Bed Grade' and 'City Code Patient' both have missings values. We can replace the 'Bed Grade' missing values with the median. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bBiNLmJzDokB"
      },
      "source": [
        "train['Bed Grade'].median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7tYTNpVPDokd"
      },
      "source": [
        "train['Bed Grade'].fillna(train['Bed Grade'].median(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mRRfcl9TDoku"
      },
      "source": [
        "train['Bed Grade'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZV80lOuDolB"
      },
      "source": [
        "For 'City Code Patient', we can predict the missing values but looking at entries with similiar data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1QW0lpx9DolG"
      },
      "source": [
        "train['City_Code_Patient'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UWwtRJhxDolU"
      },
      "source": [
        "train['Hospital_type_code'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sABi1wBZDol6"
      },
      "source": [
        "train['Department'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rfnhN3RGDomT"
      },
      "source": [
        "train.groupby(['City_Code_Hospital', 'Hospital_type_code','Department'])['City_Code_Patient'].agg(pd.Series.mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZzGEOCm-Domg"
      },
      "source": [
        "train['City_Code_Patient'] = train.groupby(['City_Code_Hospital', 'Hospital_type_code','Department'], sort=False)['City_Code_Patient'].apply(lambda x: x.fillna(x.value_counts().index[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AwTf8NUhDomy"
      },
      "source": [
        "train['City_Code_Patient'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "o0Io6mEhDom-"
      },
      "source": [
        "train.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mjSbzNq7DonM"
      },
      "source": [
        "cat_columns = ['Hospital_code', 'Hospital_type_code', 'City_Code_Hospital', 'Hospital_region_code',\n",
        "               'Department', 'Ward_Type', 'Ward_Facility_Code', 'City_Code_Patient']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d8E3qVL_DonZ"
      },
      "source": [
        "train = train.astype({'Hospital_code':'category', 'Hospital_type_code':'category', \n",
        "                     'City_Code_Hospital':'category', \n",
        "                     'Hospital_region_code':'category','Department':'category', \n",
        "                     'Ward_Type':'category', 'Ward_Facility_Code':'category', \n",
        "                     'City_Code_Patient':'category'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "V4HF-kwdDono"
      },
      "source": [
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DeqI0EZ2Don8"
      },
      "source": [
        "train_one_hot = pd.get_dummies(train, columns = cat_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XbYfUfjmDood"
      },
      "source": [
        "train_one_hot.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "a8l5V7J-DopA"
      },
      "source": [
        "stay_encode = {'0-10': 0,\n",
        "               '11-20': 1,\n",
        "               '21-30': 2,\n",
        "               '31-40': 3,\n",
        "               '41-50': 4,\n",
        "               '51-60': 5,\n",
        "               '61-70': 6,\n",
        "               '71-80': 7,\n",
        "               '81-90': 8,\n",
        "               '91-100': 9,\n",
        "               'More than 100 Days': 10}\n",
        "\n",
        "admission_type_encode = {'Trauma': 0, \n",
        "                         'Emergency': 1, \n",
        "                         'Urgent': 2}\n",
        "\n",
        "illness_encode = {'Minor': 0,\n",
        "                  'Moderate': 1,\n",
        "                  'Extreme': 2}\n",
        "\n",
        "age_encode = {'0-10': 0,\n",
        "              '11-20': 1,\n",
        "              '21-30': 2,\n",
        "              '31-40': 3,\n",
        "              '41-50': 4,\n",
        "              '51-60': 5,\n",
        "              '61-70': 6,\n",
        "              '71-80': 7,\n",
        "              '81-90': 8,\n",
        "              '91-100': 9}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "flQ3H7TKDopb"
      },
      "source": [
        "train_one_hot['Stay'] = train_one_hot['Stay'].map(stay_encode)\n",
        "train_one_hot['Type of Admission'] = train_one_hot['Type of Admission'].map(admission_type_encode)\n",
        "train_one_hot['Severity of Illness'] = train_one_hot['Severity of Illness'].map(illness_encode)\n",
        "train_one_hot['Age'] = train_one_hot['Age'].map(age_encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skOfxyGIDopl"
      },
      "source": [
        "We will be removing the patientid column since it is a categorical column with many unique values, and would not help improve the model. However, we will get the amount of times a patientid shows up in the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0uzETwo4Dopq"
      },
      "source": [
        "train_one_hot['patient_deposit_mean'] = train.groupby(['patientid'])['Admission_Deposit'].transform('count')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yzy_T3BXDop1"
      },
      "source": [
        "train_one_hot.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2VtzEYGDop9"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNaiLTfrDop_"
      },
      "source": [
        "We do not need to do any feature engineering because the neural net model will do the feature engineering for us. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGZ1oIrSDoqC"
      },
      "source": [
        "# Baseline Model\n",
        "We will now make a baseline neural net model that we can iterate from. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ScTZdeFYDoqF"
      },
      "source": [
        "y = train_one_hot.loc[:,['Stay']]\n",
        "X = train_one_hot.drop(['Stay','case_id', 'patientid'], axis=1)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X=pd.DataFrame(X)\n",
        "y = pd.get_dummies(y, columns=['Stay'])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kgIm-CESDoqP"
      },
      "source": [
        "def create_nn(num_cols):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(num_cols),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(11, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoSBj7_DDoqc"
      },
      "source": [
        "Let's now check if our model is working. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l3FXc9zGDoqd"
      },
      "source": [
        "model = create_nn(X_train.shape[1])\n",
        "model.fit(X_train,y_train,validation_data=(X_val, y_val), epochs=20, batch_size=64, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnAjNBocDoqo"
      },
      "source": [
        "Now that be know our model is working, we need to create the validation schema. We will use the same schema as the one we create with Catboost. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K369--CzDoqq"
      },
      "source": [
        "# Improving the model\n",
        "Now that we have our baseline model working, it is now time to improve it. \n",
        "I will test the model with the following architecture to see if performance improves. \n",
        "\n",
        "1. Dropout\n",
        "2. Batch Normalization\n",
        "3. Weight Normalization\n",
        "4. lookahead\n",
        "5. Different amount of layers\n",
        "6. Different amount of units for each layer\n",
        "7. Ealry Stopping\n",
        "8. Checkpoints\n",
        "9. Reducing learning rate using ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvmSXj7yDoqu"
      },
      "source": [
        "\n",
        "## Architecture Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vSqEYc1zDoqv"
      },
      "source": [
        "# used for splitting using StratifiedKFold\n",
        "# StratifiedKFold cannot split with one-hot encoded labels\n",
        "skf_y = train_one_hot.loc[:,['Stay']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Gsu-3FDqDoq3"
      },
      "source": [
        "def create_nn2(num_columns, hidden_units, dropout_rate):\n",
        "    \n",
        "    inp = tf.keras.layers.Input(shape = (num_columns, ))\n",
        "    x = tf.keras.layers.BatchNormalization()(inp)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    \n",
        "    for units in hidden_units:\n",
        "        \n",
        "        x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units, activation = 'relu'))(x)\n",
        "        x = tf.keras.layers.BatchNormalization()(x)\n",
        "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "        \n",
        "    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(11, activation = 'softmax'))(x)\n",
        "    \n",
        "    model = tf.keras.models.Model(inputs = inp, outputs = out)\n",
        "    \n",
        "    model.compile(optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam()), \n",
        "                  loss='categorical_crossentropy', \n",
        "                  metrics = ['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "u4VRSV7bDorA"
      },
      "source": [
        "def optimise(params):\n",
        "    \n",
        "    print(params)\n",
        "    \n",
        "    N_SPLITS=2\n",
        "    \n",
        "    cv_result = y.copy()\n",
        "    cv_result.loc[:, y.columns] = 0\n",
        "    \n",
        "    historys = {}\n",
        "\n",
        "    skf = StratifiedKFold(n_splits = N_SPLITS, random_state = RANDOM_STATE, shuffle = True)\n",
        "\n",
        "    for n, (train_ind, val_ind) in enumerate(skf.split(skf_y, skf_y)):\n",
        "\n",
        "        print('Fold', n)\n",
        "\n",
        "        x_tr, x_val = X.values[train_ind], X.values[val_ind]\n",
        "        y_tr, y_val = y.values[train_ind], y.values[val_ind]\n",
        "        \n",
        "        model = create_nn2(X.shape[1], [params['hidden_unit_1'], params['hidden_unit_1'], params['hidden_unit_1']],\n",
        "                                  params['dropout_rate'])\n",
        "\n",
        "        checkpoint_path = f'Fold:{n}.hdf5'\n",
        "        \n",
        "        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-5, \n",
        "                                           patience=3, verbose=0, mode='min')\n",
        "\n",
        "        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
        "                                     verbose = 0, save_best_only = True, \n",
        "                                     save_weights_only = True, mode = 'min')\n",
        "\n",
        "        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
        "                              restore_best_weights=True, \n",
        "                              patience= 5, verbose = 0)\n",
        "\n",
        "        history = model.fit(x_tr, y_tr, \n",
        "                            validation_data=(x_val, y_val), \n",
        "                            epochs=50, \n",
        "                            batch_size=128,\n",
        "                            callbacks=[reduce_lr_loss, cb_checkpt, early],\n",
        "                            verbose=0)\n",
        "\n",
        "        hist = pd.DataFrame(history.history)\n",
        "        \n",
        "        print('history saved')\n",
        "        model.load_weights(checkpoint_path)\n",
        "\n",
        "        cv_result.loc[val_ind, y.columns] += model.predict(x_val)\n",
        "        print(f'Fold {n} Best Loss:\\t', hist['val_loss'].min())\n",
        "\n",
        "        K.clear_session()\n",
        "        del model, history, hist\n",
        "        gc.collect()\n",
        "    \n",
        "    cv_result.loc[val_ind, y.columns] / N_SPLITS\n",
        "    score = log_loss(y.values, cv_result.values)\n",
        "    print('Total Score', score)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_1lgbC-eDorL"
      },
      "source": [
        "param_space = {'hidden_unit_1': hp.choice('hidden_unit_1', [1024, 512, 256, 128]), \n",
        "               'hidden_unit_2': hp.choice('hidden_unit_2', [1024, 512, 256, 128]), \n",
        "               'hidden_unit_3': hp.choice('hidden_unit_3', [512, 256, 128, 0]), \n",
        "               'dropout_rate': hp.uniform('dropout_rate', 0.3, 0.5), \n",
        "              }\n",
        "\n",
        "trials = Trials()\n",
        "\n",
        "hopt = fmin(fn = optimise, \n",
        "            space = param_space, \n",
        "            algo = tpe.suggest, \n",
        "            max_evals = 15, \n",
        "            timeout = 1800, \n",
        "            trials = trials, \n",
        "           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgII00GXDorT"
      },
      "source": [
        "# Improved Model\n",
        "Let's create a new improved model from the architecture tuning results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zm53kdMJDorV"
      },
      "source": [
        "def improved_nn(num_cols):\n",
        "    model = tf.keras.Sequential([      \n",
        "    tf.keras.layers.Input(num_cols),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(256, activation='relu')),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(128, activation='relu')),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tfa.layers.WeightNormalization(tf.keras.layers.Dense(11, activation='softmax'))\n",
        "    ])\n",
        "    model.compile(loss = 'categorical_crossentropy', \n",
        "                  optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam()),\n",
        "                  metrics = ['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ohjaVvyCDorf"
      },
      "source": [
        "def run_improved_nn(X, y):\n",
        "\n",
        "    N_SPLITS=5\n",
        "    N_STARTS=3 \n",
        "    \n",
        "    cv_result = y.copy()\n",
        "    cv_result.loc[:, y.columns] = 0\n",
        "    \n",
        "    for seed in range(N_STARTS):\n",
        "        skf = StratifiedKFold(n_splits = N_SPLITS, random_state = seed, shuffle = True)\n",
        "        for n, (train_ind, val_ind) in enumerate(skf.split(skf_y, skf_y)):\n",
        "\n",
        "            print(f'Seed: {seed} ------------- Fold:{n}')\n",
        "\n",
        "            x_tr, x_val = X.values[train_ind], X.values[val_ind]\n",
        "            y_tr, y_val = y.values[train_ind], y.values[val_ind]\n",
        "\n",
        "            model = improved_nn(X.shape[1])\n",
        "\n",
        "            checkpoint_path = f'Seed:{seed}-Fold:{n}.hdf5'\n",
        "\n",
        "            reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-5, \n",
        "                                               patience=3, verbose=0, mode='min')\n",
        "\n",
        "            cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', \n",
        "                                         verbose = 0, save_best_only = True, \n",
        "                                         save_weights_only = True, mode = 'min')\n",
        "\n",
        "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
        "                                  restore_best_weights=True, \n",
        "                                  patience= 5, verbose = 0)\n",
        "\n",
        "            history = model.fit(x_tr, y_tr, \n",
        "                                validation_data=(x_val, y_val), \n",
        "                                epochs=50, \n",
        "                                batch_size=128,\n",
        "                                callbacks=[reduce_lr_loss, cb_checkpt, early],\n",
        "                                verbose=2)\n",
        "\n",
        "            hist = pd.DataFrame(history.history)\n",
        "\n",
        "            print('history saved')\n",
        "            model.load_weights(checkpoint_path)\n",
        "\n",
        "            cv_result.loc[val_ind, y.columns] += model.predict(x_val)\n",
        "            print(f'Fold {n} Best Loss:\\t', hist['val_loss'].min())\n",
        "\n",
        "            K.clear_session()\n",
        "            del model, history, hist\n",
        "            gc.collect()\n",
        "    \n",
        "    cv_result.loc[val_ind, y.columns] =/ (N_STARTS)\n",
        "    score = log_loss(y.values, cv_result.values)\n",
        "    print('Total Score', score)\n",
        "    return cv_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hyk_JWn8Dorm"
      },
      "source": [
        "cs_result = run_final_nn(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}